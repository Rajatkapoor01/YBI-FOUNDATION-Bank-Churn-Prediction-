{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPVSRZ9M4CEluNgoyaqo4GW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"lpu9QidxTIyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**YBI FOUNDATION PROJECT-1**\n","**TITLE OF THE PROJECT**\n","\n","*Customer Clustering and Churn Prediction in a Bank*"],"metadata":{"id":"im77NidccJyc"}},{"cell_type":"code","source":["#The objective of this project is to cluster bank customers and predict customer churn using machine learning techniques."],"metadata":{"id":"MrHSgAbDMgGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DATA SOURCE\n","#The dataset for this project is available on Kaggle: Bank Customer Churn Modeling.\n","# https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling"],"metadata":{"id":"hocrGuiWcTv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#IMPORT LIBRARIES\n","\n","import numpy as np\n","import scipy as sc\n","import sklearn as sk\n","import pandas as pd\n","import seaborn as sb\n","import random\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.patches as mpatches\n","import seaborn as sns\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score, silhouette_samples\n","from matplotlib.ticker import MaxNLocator\n","from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n","\n","pd.options.mode.chained_assignment = None\n"],"metadata":{"id":"V0ichtQOcWgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#IMPORT DATA\n","\n","# Loading the .csv\n","# Loading the .csv\n","bank_data = pd.read_csv(\"/content/churn_dataset_Bank.csv\")\n","\n"],"metadata":{"id":"cgijIgWecYK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DATA PREPROCESSING\n","# Eliminating unnecessary attributes\n","bank_data.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\n","\n","# Renaming and encoding the 'Gender' column\n","bank_data.rename(columns={'Gender': 'IsMale'}, inplace=True)\n","bank_data['IsMale'] = bank_data['IsMale'].apply(lambda x: 1 if x == 'Male' else 0)\n","\n","# Separating numerical and categorical variables\n","num_subset = bank_data.select_dtypes('number')\n","cat_subset = bank_data.select_dtypes('object')\n","\n","# One-hot encoding categorical variables\n","cat_subset = pd.get_dummies(cat_subset)\n","\n","# Saving a denormalized but organized version of the dataset\n","denorm_bank_data = pd.concat([cat_subset, num_subset], axis=1)\n","\n","# Normalizing numerical variables\n","maxvals = num_subset.astype(float).max()\n","numericalColumns = {'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary'}\n","for col in numericalColumns:\n","    num_subset[col] = num_subset[col] / maxvals[col]\n","bank_data = pd.concat([cat_subset, num_subset], axis=1)\n","\n","# Printing dataset types and displaying dataset head\n","print(bank_data.dtypes)\n","display(bank_data.head())\n"],"metadata":{"id":"yAVQbQhbKdju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define Target Variable (Y) and Feature Variables (X):\n","\n","# Defining X and Y\n","bankX = bank_data.iloc[:, :12]\n","bankY = bank_data.iloc[:, 12:13]\n","\n","X = bankX.values  # numpy array (10000, 12)\n","Y = bankY.values  # numpy array (10000, 1)\n"],"metadata":{"id":"CM_B4i8QJfr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train Test Split\n","\n","origDataModel = []  # will contain the #iterations models trained with the original (imbalanced) data\n","\n","for i in range(iterations):\n","\n","  print(\"Iteration Nº\", i, \": \\n\")\n","\n","  X_train, X_test, Y_train, Y_test = train_test_split(X, Y.ravel(), test_size=0.3)\n","\n","  model = tf.keras.Sequential()\n","\n","  model.add(Dense(256, activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","\n","  model.add(Dense(128,  activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","\n","  model.add(Dense(64, activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","\n","  model.add(Dense(32, activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","\n","  model.add(Dense(8,  activation='relu'))\n","  model.add(BatchNormalization())\n","  model.add(Dropout(0.3))\n","\n","  model.add(Dense(1, activation='sigmoid'))\n","\n","  model.compile(optimizer=tf.train.AdamOptimizer(),\n","                loss='binary_crossentropy',\n","                metrics=['acc'])\n","\n","  model_logs = model.fit(X_train,\n","          Y_train,\n","          batch_size=32,\n","          epochs=120,\n","          verbose=0,  # silent mode\n","          validation_data=(X_test, Y_test))  # Only to check that the model is not \"overfitting\" the training data\n","\n","  origDataModel.append(model)\n","\n","    # Pseudo-validate with common validation data\n","\n","  score = model.evaluate(X_val, Y_val, verbose=0)  # tensorflow default threshold = 0.5\n","  print(\"Accuracy training with imbalanced data (default threshold=0.5): {:-5f} %\".format(score[1] * 100))\n","\n","  Y_pred = model.predict(X_val) > 0.5  # manual threshold\n","  matConf = confusion_matrix(Y_val, Y_pred)\n","  valsize = Y_val.shape[0]\n","\n","  plt.figure(figsize=(6, 5))  # Establishing the heatmap size before plotting\n","  ax = sb.heatmap(matConf, annot=True, fmt=\".0f\")\n","  ax.set_ylabel('Original', fontsize=15)\n","  ax.set_xlabel('Predicted', fontsize=15)\n","  plt.title(\"Imbalanced data\")\n","  plt.show()"],"metadata":{"id":"_0k7Yr35OP05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Data Visualization:\n","\n","list_binary = [0, 1, 2, 4, 9, 10]\n","list_normal = [3, 5, 6, 7, 8, 11]\n","order = [0, 1, 2, 4, 9, 10, 8, 6, 5, 3, 7, 11]\n","\n","fig = plt.figure(figsize=(20, 8))\n","for i in range(len(order)):\n","    xi = denorm_bank_data.values[:, order[i]]\n","    # Use integer division // to ensure an integer number of columns\n","    ax1 = fig.add_subplot(2, len(order) // 2, i + 1)\n","    plt.title(list(bank_data)[order[i]], fontsize=16)\n","    if i < 6:\n","        # Convert boolean values to numerical (0 and 1) for histogram\n","        plt.hist(xi.astype(int), 2)\n","        plt.xticks([0.25, 0.75], np.arange(0, 2, 1))\n","    else:\n","        plt.hist(xi, 100)\n","plt.suptitle('Distributions of the Variables', fontsize=30)\n","plt.show()"],"metadata":{"id":"i-ySFZ7rNAqO"},"execution_count":null,"outputs":[]},{"source":["#Train-Test Split and Model Training:\n","\n","iterations = 10\n","origDataModel = []\n","\n","for i in range(iterations):\n","    print(f\"Iteration Nº {i}: \\n\")\n","\n","    # Convert X and Y to float32 before splitting\n","    X_train, X_test, Y_train, Y_test = train_test_split(X.astype(np.float32),\n","                                                        Y.ravel().astype(np.float32),\n","                                                        test_size=0.3)\n","\n","    model = tf.keras.Sequential()\n","\n","    model.add(Dense(256, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(128, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(64, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(32, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(8, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(),\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    model_logs = model.fit(X_train, Y_train, batch_size=32, epochs=120, verbose=0,\n","                           validation_data=(X_test, Y_test))\n","\n","    origDataModel.append(model)\n","\n","    # Pseudo-validate with common validation data\n","    score = model.evaluate(X_test, Y_test, verbose=0)\n","    print(f\"Accuracy training with imbalanced data (default threshold=0.5): {score[1] * 100:.2f} %\")\n","\n","    Y_pred = model.predict(X_test) > 0.5\n","    matConf = confusion_matrix(Y_test, Y_pred)\n","\n","    plt.figure(figsize=(6, 5))\n","    ax = sb.heatmap(matConf, annot=True, fmt=\".0f\")\n","    ax.set_ylabel('Original', fontsize=15)\n","    ax.set_xlabel('Predicted', fontsize=15)\n","    plt.title(\"Imbalanced data\")\n","    plt.show()"],"cell_type":"code","metadata":{"collapsed":true,"id":"xtZbW-WgPnOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# **What is the proportion of customers for each nationality?**\n","\n","fig=plt.figure(figsize=(5,5))\n","# First, we will find how many customers each country has\n","Customers_France = bankX.Geography_France.sum()\n","Customers_Germany = bankX.Geography_Germany.sum()\n","Customers_Spain = bankX.Geography_Spain.sum()\n","\n","# We label, color and plot our data\n","labels = ['Germany','Spain','France']\n","sizes = [Customers_Germany, Customers_Spain,Customers_France]\n","colors = ['lightcoral','gold', 'cadetblue']\n","plt.title('Nationality - Proportion', fontsize=20)\n","plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140,textprops={'fontsize': 14})\n","plt.axis('equal')\n","plt.show()"],"metadata":{"id":"rAud65rXQnPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What is the GENDER Ratio:\n","\n","fig=plt.figure(figsize=(5,5))\n","# Data to plot\n","Men = bankX.loc[bankX['IsMale']==1, 'IsMale'].count()\n","Women = bankX.loc[bankX['IsMale']==0, 'IsMale'].count()\n","labels = ['Men', 'Women']\n","sizes = [Men,Women]\n","colors = ['#5539cc','#cb416b']\n","plt.title('Gender - Proportion', fontsize=20)\n","plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90,textprops={'fontsize': 14})\n","plt.axis('equal')\n","plt.show()"],"metadata":{"id":"46I1bDTLRR5E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Correlation between the variable\n","\n","fig=plt.figure(figsize=(10,10))\n","CX=bank_data.corr()\n","# Use bool instead of np.bool\n","mask = np.zeros_like(CX, dtype=bool)\n","mask[np.triu_indices_from(mask)] = True\n","heat=sb.heatmap(CX,mask=mask,annot=True, vmin=-1, vmax=1, fmt='.2f',cmap='RdBu_r')\n","fig.add_subplot(heat)\n","plt.show()"],"metadata":{"id":"BMbRsvtcRlQz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Project Summary: Customer Clustering and Churn Prediction in a Bank**\n","\n","#### **Objective:**\n","The main goal of this project is to segment bank customers and predict customer churn using machine learning techniques. By identifying patterns and predicting churn, banks can devise strategies to retain customers and improve their services.\n","\n","#### **Data Source:**\n","The dataset used for this project is available on Kaggle: [Bank Customer Churn Modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling).\n","\n","#### **Libraries and Tools:**\n","- **Data Manipulation and Analysis:** NumPy, Pandas, SciPy\n","- **Visualization:** Matplotlib, Seaborn\n","- **Machine Learning:** Scikit-learn, TensorFlow, Keras\n","- **Miscellaneous:** Random, mpl_toolkits.mplot3d, Matplotlib patches\n","\n","#### **Data Preprocessing:**\n","1. **Loading the Dataset:**\n","   - The dataset is loaded from a CSV file.\n","2. **Dropping Unnecessary Columns:**\n","   - Columns like `RowNumber`, `CustomerId`, and `Surname` are dropped.\n","3. **Renaming and Encoding:**\n","   - The `Gender` column is renamed to `IsMale` and encoded to binary (1 for Male, 0 for Female).\n","4. **Separating Numerical and Categorical Variables:**\n","   - Numerical variables and categorical variables are separated for further processing.\n","5. **One-Hot Encoding:**\n","   - Categorical variables are one-hot encoded.\n","6. **Normalization:**\n","   - Numerical variables are normalized to a range between 0 and 1.\n","\n","#### **Data Visualization:**\n","- **Distribution of Variables:** Histograms are used to visualize the distribution of various numerical and categorical variables.\n","- **Nationality Proportion:** A pie chart is used to show the proportion of customers from Germany, Spain, and France.\n","- **Gender Ratio:** A pie chart displays the gender distribution among customers.\n","- **Correlation Matrix:** A heatmap is used to visualize the correlation between different variables in the dataset.\n","\n","#### **Model Training:**\n","1. **Defining Features and Target Variable:**\n","   - The features (`X`) and the target variable (`Y`) are defined from the processed dataset.\n","2. **Train-Test Split:**\n","   - The dataset is split into training and testing sets.\n","3. **Model Architecture:**\n","   - A neural network model is built using TensorFlow and Keras with multiple layers (Dense, BatchNormalization, Dropout).\n","4. **Model Compilation and Training:**\n","   - The model is compiled with the Adam optimizer and trained using binary cross-entropy loss.\n","5. **Model Evaluation:**\n","   - The model's performance is evaluated using accuracy, and a confusion matrix is plotted for further insights.\n","\n","#### **Key Insights:**\n","1. **Nationality Distribution:**\n","   - The majority of customers are from France, followed by Germany and Spain.\n","2. **Gender Distribution:**\n","   - The dataset contains more male customers compared to female customers.\n","3. **Correlation Analysis:**\n","   - The heatmap reveals correlations between different features, helping to understand which features are strongly related.\n","\n","#### **Model Performance:**\n","- **Accuracy:** The model's accuracy is evaluated after each iteration of training, with results indicating the model's capability to predict churn accurately.\n","- **Confusion Matrix:** The confusion matrix helps in understanding the model's performance in terms of true positives, true negatives, false positives, and false negatives.\n","\n","#### **Conclusion:**\n","This project demonstrates how to preprocess a dataset, visualize key insights, and build a machine learning model to predict customer churn. By understanding customer segments and predicting churn, banks can implement targeted strategies to retain customers and enhance their overall experience. Further model evaluation metrics like precision, recall, F1 score, and ROC-AUC curves can be used to gain deeper insights into the model's performance."],"metadata":{"id":"EyyCY9d7SvHC"}}]}